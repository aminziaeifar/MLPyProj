{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Ebnable HTML/CSS \n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Enter Team Member Names here (double click to edit):\n",
    "\n",
    "- Name 1: Hedieh Ashrafi\n",
    "- Name 2: Javad Jomepour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class Assignment One\n",
    "In the following assignment you will be asked to fill in python code and derivations for a number of different problems. Please read all instructions carefully and turn in the rendered notebook (or HTML of the rendered notebook)  before the end of class (or right after class). The initial portion of this notebook is given before class and the remainder is given during class. Please answer the initial questions before class, to the best of your ability. Once class has started you may rework your answers as a team for the initial part of the assignment. \n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Contents\n",
    "* <a href=\"#Loading\">Loading the Data</a>\n",
    "* <a href=\"#linearnumpy\">Linear Regression</a>\n",
    "* <a href=\"#sklearn\">Using Scikit Learn for Regression</a>\n",
    "* <a href=\"#classification\">Linear Classification</a>\n",
    "\n",
    "________________________________________________________________________________________________________\n",
    "\n",
    "<a id=\"Loading\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Loading the Data\n",
    "Please run the following code to read in the \"diabetes\" dataset from sklearn's data loading module. \n",
    "\n",
    "This will load the data into the variable `ds`. `ds` is a `bunch` object with fields like `ds.data` and `ds.target`. The field `ds.data` is a numpy matrix of the continuous features in the dataset. **The object is not a pandas dataframe. It is a numpy matrix.** Each row is a set of observed instances, each column is a different feature. It also has a field called `ds.target` that is a continuous value we are trying to predict. Each entry in `ds.target` is a label for each row of the `ds.data` matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (442, 10) format is: ('rows', 'columns')\n",
      "range of target: 25.0 346.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # needs to be at the top of this cell\n",
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "\n",
    "ds = load_diabetes()\n",
    "\n",
    "# this holds the continuous feature data\n",
    "# because ds.data is a matrix, there are some special properties we can access (like 'shape')\n",
    "print('features shape:', ds.data.shape, 'format is:', ('rows','columns')) # there are 442 instances and 10 features per instance\n",
    "print('range of target:', np.min(ds.target),np.max(ds.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
      "         0.01990842, -0.01764613],\n",
      "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
      "        -0.06832974, -0.09220405],\n",
      "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
      "         0.00286377, -0.02593034],\n",
      "       ...,\n",
      "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
      "        -0.04687948,  0.01549073],\n",
      "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
      "         0.04452837, -0.02593034],\n",
      "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
      "        -0.00421986,  0.00306441]])\n",
      "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
      "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
      "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
      "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
      "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
      "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
      "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
      "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
      "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
      "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
      "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
      "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
      "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
      "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
      "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
      "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
      "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
      "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
      "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
      "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
      "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
      "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
      "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
      "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
      "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
      "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
      "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
      "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
      "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
      "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
      "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
      "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
      "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
      "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
      "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
      "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
      "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
      "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
      "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
      "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
      "       220.,  57.])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# we can set the fields inside of ds and set them to new variables in python\n",
    "pprint(ds.data) # prints out elements of the matrix\n",
    "pprint(ds.target) # prints the vector (all 442 items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"linearnumpy\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Using Linear Regression \n",
    "In the videos, we derived the formula for calculating the optimal values of the regression weights (you must be connected to the internet for this equation to show up properly):\n",
    "\n",
    "$$ w = (X^TX)^{-1}X^Ty $$\n",
    "\n",
    "where $X$ is the matrix of values with a bias column of ones appended onto it. For the diabetes dataset one could construct this $X$ matrix by stacking a column of ones onto the `ds.data` matrix. \n",
    "\n",
    "$$ X=\\begin{bmatrix}\n",
    "         & \\vdots &        &  1 \\\\\n",
    "        \\dotsb & \\text{ds.data} & \\dotsb &  \\vdots\\\\\n",
    "         & \\vdots &         &  1\\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Question 1:** For the diabetes dataset, how many elements will the vector $w$ contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -10.01219782 -239.81908937  519.83978679  324.39042769 -792.18416163\n",
      "  476.74583782  101.04457032  177.06417623  751.27932109   67.62538639\n",
      "  152.13348416]\n",
      "(11,)\n",
      "\n",
      "The weight vector 'w' will have  11  elements.\n"
     ]
    }
   ],
   "source": [
    "# Enter your answer here (or write code to calculate it)\n",
    "from numpy import linalg as lin\n",
    "\n",
    "bias = np.ones((ds.data.shape[0], 1))\n",
    "X = np.concatenate((ds.data, bias), axis=1)\n",
    "y = ds.target\n",
    "w = lin.inv(X.T @ X) @ X.T @ y\n",
    "print(w)\n",
    "print(w.shape)\n",
    "print(\"\\nThe weight vector 'w' will have \", w.shape[0], \" elements.\")\n",
    "\n",
    "# the weight vector 'w' will have 11 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "**Exercise 1:** In the following empty cell, use the given equation above (using numpy matrix operations) to find the values of the optimal vector $w$. You will need to be sure $X$ and $y$ are created like the instructor talked about in the video. Don't forget to include any modifications to $X$ to account for the bias term in $w$. You might be interested in the following functions:\n",
    "\n",
    "- `import numpy as np`\n",
    "- `np.hstack((mat1,mat2))` stack two matrices horizontally, to create a new matrix\n",
    "- `np.ones((rows,cols))` create a matrix full of ones\n",
    "- `my_mat.T` takes transpose of numpy matrix named `my_mat`\n",
    "- `np.dot(mat1,mat2)` or `mat1 @ mat2` is matrix multiplication for two matrices\n",
    "- `np.linalg.inv(mat)` gets the inverse of the variable `mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write you code here, print the values of the regression weights using the 'print()' function in python\n",
    "\n",
    "# NOTE: see code above for Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "<a id=\"sklearn\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "# Start of Live Session Coding\n",
    "\n",
    "**Exercise 2:** Scikit-learn also has a linear regression fitting implementation. Look at the scikit learn API and learn to use the linear regression method. The API is here: \n",
    "\n",
    "- API Reference: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Use the sklearn `LinearRegression` module to check your results from the previous question. \n",
    "\n",
    "**Question 2**: Did you get the same parameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous weights:\n",
      " [ -10.01219782 -239.81908937  519.83978679  324.39042769 -792.18416163\n",
      "  476.74583782  101.04457032  177.06417623  751.27932109   67.62538639\n",
      "  152.13348416]\n",
      "\n",
      "Model coefficients are:\n",
      " [ -10.01219782 -239.81908937  519.83978679  324.39042769 -792.18416163\n",
      "  476.74583782  101.04457032  177.06417623  751.27932109   67.62538639\n",
      "    0.        ]\n",
      "\n",
      "Difference:\n",
      " [6.28830321e-13 3.12638804e-13 5.68434189e-13 7.38964445e-13\n",
      " 3.52429197e-12 5.11590770e-13 4.10693701e-12 2.18847163e-12\n",
      " 2.38742359e-12 3.69482223e-13 1.52133484e+02]\n",
      "\n",
      "Model intercept is: 152.1334841628965\n",
      "\n",
      "Answer to question is: \n",
      "      The parameters obtained with Scikit-learn are all the same except for the last entry, which corresponds to the\n",
      "      added bias column. The value for this entry in the weight vector calculated in Exercise 1 matches the intercept\n",
      "      calculated by Scikit-learn's linear regression.\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# write your code here, print the values of model by accessing \n",
    "#    its properties that you looked up from the API\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "w_diff = np.absolute(w - reg.coef_)\n",
    "\n",
    "print('Previous weights:\\n', w)\n",
    "print('\\nModel coefficients are:\\n', reg.coef_)\n",
    "print('\\nDifference:\\n', w_diff)\n",
    "print('\\nModel intercept is:', reg.intercept_)\n",
    "print('\\nAnswer to question is:',\n",
    "      \"\"\"\n",
    "      The parameters obtained with Scikit-learn are all the same except for the last entry, which corresponds to the\n",
    "      added bias column. The value for this entry in the weight vector calculated in Exercise 1 matches the intercept\n",
    "      calculated by Scikit-learn's linear regression.\n",
    "      \"\"\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "Recall that to predict the output from our model, $\\hat{y}$, from $w$ and $X$ we need to use the following formula:\n",
    "\n",
    "- $\\hat{y}=w^TX^T$, for row vector $\\hat{y}$\n",
    "- OR \n",
    "- $\\hat{y}=Xw$, for column vector $\\hat{y}$\n",
    "\n",
    "Where $X$ is a matrix with example instances in *each row* of the matrix (and the bias term).\n",
    "\n",
    "**Exercise 3:** \n",
    "- *Part A:* Use matrix multiplication to predict output using numpy, $\\hat{y}_{numpy}$. \n",
    " - **Note**: you may need to make the regression weights a column vector using the following code: `w = w.reshape((len(w),1))` This assumes your weights vector is assigned to the variable named `w`.\n",
    "- *Part B:* Use the sklearn API to get the values for $\\hat{y}_{sklearn}$ (hint: use the `.predict` function of the API).\n",
    "- *Part C:* Calculate the mean squared error between your prediction from numpy and the target, $\\frac{1}{M}\\sum_i(y-\\hat{y}_{numpy})^2$. \n",
    "- *Part D:* Calculate the mean squared error between your sklearn prediction and the target, $\\frac{1}{M}\\sum_i(y-\\hat{y}_{sklearn})^2$.\n",
    " - **Note**: parts C and D can each be completed in one line of code using numpy. There is no need to write a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Sklearn is: 2859.69\n",
      "MSE Numpy is:   2859.69\n"
     ]
    }
   ],
   "source": [
    "# Use this block to answer the questions\n",
    "w = w.reshape((len(w),1)) # make w a column vector\n",
    "y = y.reshape((len(y),1)) # make y a column vector\n",
    "\n",
    "yhat_numpy = X @ w\n",
    "yhat_sklearn = reg.predict(X)\n",
    "yhat_sklearn = yhat_sklearn.reshape((len(yhat_sklearn),1)) # make yhat_sklearn a column vector\n",
    "\n",
    "MSE_numpy = (1/len(y)) * np.sum((y-yhat_numpy)**2)\n",
    "MSE_sklearn = (1/len(y)) * np.sum((y-yhat_sklearn)**2)\n",
    "\n",
    "print('MSE Sklearn is: {0:1.2f}'.format(MSE_sklearn))\n",
    "print('MSE Numpy is:   {0:1.2f}'.format(MSE_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"classification\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Using Linear Classification\n",
    "Now lets use the code you created to make a classifier with linear boundaries. Run the following code in order to load the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (150, 4)\n",
      "original number of classes: 3\n",
      "new number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# this will overwrite the diabetes dataset\n",
    "ds = load_iris()\n",
    "print('features shape:', ds.data.shape) # there are 150 instances and 4 features per instance\n",
    "print('original number of classes:', len(np.unique(ds.target)))\n",
    "\n",
    "# now let's make this a binary classification task\n",
    "ds.target = ds.target>1\n",
    "print ('new number of classes:', len(np.unique(ds.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "**Exercise 4:** Now use linear regression to come up with a set of weights, `w`, that predict the class value. You can use numpy or sklearn, whichever you prefer. This is exactly like you did before for the *diabetes* dataset. However, instead of regressing to continuous values, you are just regressing to the integer value of the class (0 or 1), like we talked about in the video (using the hard limit function). \n",
    " - **Note**: If you are using numpy, remember to account for the bias term when constructing the feature matrix, `X`.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy model coefficients are:\n",
      " [-0.04587608  0.20276839  0.00398791  0.55177932 -0.69528186]\n",
      "\n",
      "Sklearn model coefficients are:\n",
      " [-0.04587608  0.20276839  0.00398791  0.55177932  0.        ]\n",
      "\n",
      "Sklearn model intercept is: -0.6952818633256028\n"
     ]
    }
   ],
   "source": [
    "# write your code here and print the values of the weights \n",
    "bias = np.ones((ds.data.shape[0], 1))\n",
    "X = np.concatenate((ds.data, bias), axis=1)\n",
    "y = ds.target\n",
    "w = lin.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "# Print the weights of the linear classifier.\n",
    "print('Numpy model coefficients are:\\n', w)\n",
    "print('\\nSklearn model coefficients are:\\n', reg.coef_)\n",
    "print('\\nSklearn model intercept is:', reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "**Exercise 5:** Finally, use a hard decision function on the output of the linear regression to make this a binary classifier. This is just like we talked about in the video, where the output of the linear regression passes through a function: \n",
    "\n",
    "- $\\hat{y}=g(w^TX^T)$ where\n",
    " - $g(w^TX^T)$ for $w^TX^T < \\alpha$ maps the predicted class to `0` \n",
    " - $g(w^TX^T)$ for $w^TX^T \\geq \\alpha$ maps the predicted class to `1`. \n",
    "\n",
    "Here, alpha is a threshold for deciding the class. \n",
    "\n",
    "**Question 3**: What value for $\\alpha$ makes the most sense? What is the accuracy of the classifier given the $\\alpha$ you chose? \n",
    "\n",
    "Note: You can calculate the accuracy with the following code: `accuracy = float(sum(yhat==y)) / len(y)` assuming you choose variable names `y` and `yhat` for the target and prediction, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy percentage accuracy:   92.67%\n",
      "Sklearn percentage accuracy: 92.67%\n",
      "\n",
      "Answer to question is: \n",
      "      A box plot was used to help visualize the distribution of predictions for both target classifications. Based on\n",
      "      these plots, an alpha value of 0.5 was chosen as it closely represents the midpoint between the bulk of the\n",
      "      prediction values for the \"True\" classifications and the \"False\" classifications. There are some outliers for\n",
      "      both classification types that end up causing incorrect predictions, however the accuracy for both Numpy and\n",
      "      Sklearn using this method is 92.67%.\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# predict the classification output using a hard decision function\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True) # one of the many color mappings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "yhat_numpy = X @ w\n",
    "yhat_sklearn = reg.predict(X)\n",
    "\n",
    "# use box plots to help select appropriate alpha\n",
    "plt.subplots(figsize=(20,8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot(x=y, y=yhat_numpy, hue=None)\n",
    "plt.title('Numpy')\n",
    "plt.ylabel('Numpy prediction')\n",
    "plt.xlabel('Target')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.boxplot(x=y, y=yhat_sklearn, hue=None)\n",
    "plt.title('Sklearn')\n",
    "plt.ylabel('Sklearn prediction')\n",
    "plt.xlabel('Target')\n",
    "plt.show()\n",
    "\n",
    "# apply hard decision function\n",
    "alpha = 0.5\n",
    "yhat_numpy = yhat_numpy>=alpha\n",
    "yhat_sklearn = yhat_sklearn>=alpha\n",
    "\n",
    "accuracy_numpy = float(sum(yhat_numpy==y)) / len(y)\n",
    "accuracy_sklearn = float(sum(yhat_sklearn==y)) / len(y)\n",
    "\n",
    "print('Numpy percentage accuracy:   {0:1.2f}%'.format(accuracy_numpy*100.0))\n",
    "print('Sklearn percentage accuracy: {0:1.2f}%'.format(accuracy_sklearn*100.0))\n",
    "print('\\nAnswer to question is:',\n",
    "      \"\"\"\n",
    "      A box plot was used to help visualize the distribution of predictions for both target classifications. Based on\n",
    "      these plots, an alpha value of 0.5 was chosen as it closely represents the midpoint between the bulk of the\n",
    "      prediction values for the \"True\" classifications and the \"False\" classifications. There are some outliers for\n",
    "      both classification types that end up causing incorrect predictions, however the accuracy for both Numpy and\n",
    "      Sklearn using this method is 92.67%.\n",
    "      \"\"\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "That's all! Please **save (make sure you saved!!!) and upload your rendered notebook** and please include **team member names** in the notebook submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
